install.packages("rafalib")
library(rafalib)
install.packages("swirl")
library(swirl)
swirl
swirl()
5 + 7
x = 5 + 7
x <- 5 + 7
x
y <- x - 3
y
z <- c(1.1, 9, 3.14)
?c
z
c(z, 555, z)
z * 2 + 100
my_sqrt <- sqrt(z - 1)
my_sqrt
my_div <- z / my_sqrt
my_div
c(1, 2, 3 ,4) + c(0, 10)
c(1, 2, 3, 4) + c(0, 10, 100)
z * 2 + 1000
my_div
version
clear
cl
vec1 <- c(2.23, 3.45, 1.87, 2.11, 7.33, 18.34, 19.23)
vec1
mean(vec1)
swirl()
for (i in 1:24){}
x = 0
for (i in 1:25){
x <- x + i ^ 2
}
x
cars
type(cars)
head(cars)
class(cars)
len(cars)
length(cars)
width(cars)
size(cars)
dim(cars)
col(cars)
cars[,2]
mean(cars[,2])
?which
which(cars[,2] == 85)
x = 1
x
x = 1:10
y = c(1:10)
x == y
x = 1:10
y = rnorm(10)
plot(x, y)
fit(y ~ x)
fit = lm(y ~ x)
library(dplyr)
dat = read.csv("femaleMiceWeights.csv")
list(5:9, "r")
?melt
??melt
y <- 1:4
y
attr(y, "new attribute") <- "Here"
y
y <- 1:5
y
attr(y, "new attribute")
setwd("~/DataScience/Projects/Interview - Optus/Short-Project-NASA-Web-Logs/Code")
rm(list = ls())
options(java.parameters = "-Xmx8000m")
library(dplyr)
library(data.table)
source('extract_data.R')
options(scipen = 999)
# Using extract_data function to grab data from raw NASA web log files
df = extract_data('../../Data/nasa_19950630.22-19950728.12.tsv.gz')
df2 = extract_data('../../Data/nasa_19950731.22-19950831.22.tsv.gz')
df = rbind(df, df2)
rm(df2)
# 1. Check NASA web average traffic over 24 hours-----
# Using web_traffic function to count logged activities in each hour of the day
# Then using web_traffic_viz function to visualise the frequency of logged activities
source('web_traffic.R')
web_plot = web_traffic_viz(web_traffic(df))
web_ot_plot = web_traffic_ot_viz(web_traffic_ot(df))
grid.arrange(web_plot, web_ot_plot, ncol = 1)
source('web_traffic.R')
web_plot = web_traffic_viz(web_traffic(df))
web_ot_plot = web_traffic_ot_viz(web_traffic_ot(df))
grid.arrange(web_plot, web_ot_plot, ncol = 1)
source('web_traffic.R')
web_plot = web_traffic_viz(web_traffic(df))
web_ot_plot = web_traffic_ot_viz(web_traffic_ot(df))
grid.arrange(web_plot, web_ot_plot, ncol = 1)
head(df)
strsplit(df$host[1], [.])
strsplit(df$host[1], '[.]')
lapply(strsplit(df$host[1], '[.]'), tail, n=1)
as.character(lapply(strsplit(df$host[1], '[.]'), tail, n=1))
df$host_origin = as.character(lapply(strsplit(df$host[1], '[.]'), tail, n=1))
head(df)
unique(df$host_origin)
df$host_origin = as.character(lapply(strsplit(df$host, '[.]'), tail, n=1))
head(df)
unique(df$host_origin)
table(df$host)
host_freq = as.data.table(table(df$host))
host_freq
host_freq = host_freq[with(host_freq, order(-N)), ]
host_freq
rm(host_freq)
head(df)
as.data.table(table(df$url))
url_freq = as.data.table(table(df$url))
url_freq = url_freq[with(url_freq, order(-N)), ]
url_freq
url_freq = as.data.table(table(df$url))
url_freq = url_freq[with(url_freq, order(-N)), ]
colnames(url_freq) = c("url", "freq")
url_freq$url = as.character(url_freq$url)
url_freq$freq = as.integer(url_freq$freq)
url_freq
class(url_freq)
url_freq[1:10,]
url_freq[1:100,]
head(df$url)
strsplit(df$url, '/')
head(df$url)
url_freq
url_freq[1:40,]
url_freq[100:120,]
head(df)
strsplit(df$url[1], '/')
strsplit(df$url[1], '/')[[1]]
strsplit(df$url[1], '/')[[1]][-1]
lapply(strsplit(df$url[1], '/')[[1]][-1], head, n=1)
lapply(strsplit(df$url[1], '/')[[1]], head, n=1)
strsplit(df$url[1], '/')[[1]][1]
strsplit(df$url[1], '/')[[1]][-1]
df$url[1]
substr(df$url[1], 2)
substr(df$url[1], 2, 50)
strsplit(substr(df$url[1], 2, 50), '/')[[1]][-1]
strsplit(substr(df$url[1], 2, 50), '/')[[1]]
strsplit(substr(df$url[1], 2, 50), '/')[[1]][1]
unique(strsplit(substr(df$url, 2, 50), '/')[[1]][1])
head(df$url)
strsplit(substr(df$url, 2, 50), '/')[[1]][1]
strsplit(substr(df$url, 2, 50), '/')[[1]]
head(df$url, 20)
strsplit(substr(df$url, 2, 50), '/')
strsplit(substr(df$url, 2, 50), '/')[1]
strsplit(substr(df$url, 2, 50), '/')[:]
sapply(strsplit(df$url, "/"), "[", 2)
df$root = sapply(strsplit(df$url, "/"), "[", 2)
head(df)
head(df, 20)
class(df)
root_freq = as.data.table(table(df$root))
root_freq
head(df)
df[df$root == 'yahoo.com',]
df[df$root == 'x500',]
df[df$root == 'yahoo.html',]
df[df$response >= 300 & df$response <= 399,]
nrow(df[df$response >= 300 & df$response <= 399,])
nrow(df[df$response >= 400 & df$response <= 499,])
nrow(df[df$response >= 400 & df$response <= 599,])
pop_url = function(df){
df = df[df$response < 400,]
url_freq = as.data.table(table(df$url))
url_freq = url_freq[with(url_freq, order(-N)), ]
colnames(url_freq) = c("url", "freq")
url_freq$url = as.character(url_freq$url)
url_freq$freq = as.integer(url_freq$freq)
}
url_freq = pop_url(df)
head(url_freq)
url_freq
head(df)
df[df$response < 400,]
nrow(df[df$response < 400,])
pop_url = function(df){
df = df[df$response < 400,]
url_freq = as.data.table(table(df$url))
url_freq = url_freq[with(url_freq, order(-N)), ]
colnames(url_freq) = c("url", "freq")
url_freq$url = as.character(url_freq$url)
url_freq$freq = as.integer(url_freq$freq)
}
url_freq = pop_url(df)
pop_url = function(df){
df = df[df$response < 400,]
url_freq = as.data.table(table(df$url))
url_freq = url_freq[with(url_freq, order(-N)), ]
colnames(url_freq) = c("url", "freq")
url_freq$url = as.character(url_freq$url)
url_freq$freq = as.integer(url_freq$freq)
return(url_freq)
}
url_freq = pop_url(df)
url_freq
head(df)
tail(df)
unique(df$method)
df[df$method == ]
df[df$method == 'HEAD',]
df[df$method == 'POST',]
head(pop_url)
head(df)
unique(df$root)
head(df$root)
head(df)
root_freq = as.data.table(table(df$root))
root_freq = root_freq[with(root_freq, order(-N)), ]
root_freq
colnames(root_freq) = c("url_root", "freq")
root_freq$url_root = as.character(root_freq$url_root)
root_freq$freq = as.integer(root_freq$freq)
root_freq
df[df$root == 'ksc.html',]
pop_root = function(df){
df = df[df$response < 400,]
df$root = sapply(strsplit(df$url, "/"), "[", 2)
root_freq = as.data.table(table(df$root))
root_freq = root_freq[with(root_freq, order(-N)), ]
colnames(root_freq) = c("url_root", "freq")
root_freq$url_root = as.character(root_freq$url_root)
root_freq$freq = as.integer(root_freq$freq)
return(root_freq)
}
url_freq = pop_root(df)
url_freq
pop_url = function(df){
df = df[df$response < 400,]
url_freq = as.data.table(table(df$url))
url_freq = url_freq[with(url_freq, order(-N)), ]
colnames(url_freq) = c("url", "freq")
url_freq$url = as.character(url_freq$url)
url_freq$freq = as.integer(url_freq$freq)
return(url_freq)
}
url_freq = pop_url(df)
df = df[df$response < 400,]
df$root = sapply(strsplit(df$url, "/"), "[", 2)
root_freq = as.data.table(table(df$root))
root_freq = root_freq[with(root_freq, order(-N)), ]
colnames(root_freq) = c("url_root", "freq")
root_freq$url_root = as.character(root_freq$url_root)
root_freq$freq = as.integer(root_freq$freq)
root_freq
rm(list = ls())
options(java.parameters = "-Xmx8000m")
library(dplyr)
library(data.table)
source('extract_data.R')
options(scipen = 999)
# Using extract_data function to grab data from raw NASA web log files
df = extract_data('../../Data/nasa_19950630.22-19950728.12.tsv.gz')
df2 = extract_data('../../Data/nasa_19950731.22-19950831.22.tsv.gz')
df = rbind(df, df2)
rm(df2)
# 1. Check NASA web average traffic over 24 hours-----
# Using web_traffic function to count logged activities in each hour of the day
# Then using web_traffic_viz function to visualise the frequency of logged activities
source('web_traffic.R')
web_plot = web_traffic_viz(web_traffic(df))
web_ot_plot = web_traffic_ot_viz(web_traffic_ot(df))
grid.arrange(web_plot, web_ot_plot, ncol = 1)
library(dplyr)
library(data.table)
pop_url = function(df){
df = df[df$response < 400,]
url_freq = as.data.table(table(df$url))
url_freq = url_freq[with(url_freq, order(-N)), ]
colnames(url_freq) = c("url", "freq")
url_freq$url = as.character(url_freq$url)
url_freq$freq = as.integer(url_freq$freq)
return(url_freq)
}
pop_root = function(df){
df = df[df$response < 400,]
df$root = sapply(strsplit(df$url, "/"), "[", 2)
root_freq = as.data.table(table(df$root))
root_freq = root_freq[with(root_freq, order(-N)), ]
colnames(root_freq) = c("url_root", "freq")
root_freq$url_root = as.character(root_freq$url_root)
root_freq$freq = as.integer(root_freq$freq)
return(root_freq)
}
url_freq = pop_url
url_freq = pop_url(df)
root_freq = pop_root(df)
url_freq
root_freq
url_freq
url_freq[1:10,]
ggplot(data=url_freq[1:10,], aes(x=url, y=freq)) +
geom_col(fill='#00a3ad') +
ggtitle('Top 10 URL visited') +
ylab('Count of visits') +
xlab('URLs') +
theme(axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), panel.background = element_blank(),
axis.text.x=element_text(angle=45, hjust=1))
ggplot(data=url_freq[1:10,], aes(x=url, y=freq)) +
geom_col(fill='#00a3ad') +
ggtitle('Top 10 URL visited') +
ylab('Count of visits') +
xlab('URLs') +
theme(axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), panel.background = element_blank(),
axis.text.x=element_text(angle=90, hjust=1))
ggplot(data=url_freq[1:10,], aes(x=url, y=freq)) +
geom_col(fill='#00a3ad') +
ggtitle('Top 10 URL visited') +
ylab('Count of visits') +
xlab('URLs') +
theme(axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), panel.background = element_blank(),
axis.text.x=element_text(angle=70, hjust=1))
level_order = url_freq$url[1:10,]
url_freq$url[1:10,]
url_freq
url_freq$url[1:10]
level_order = url_freq$url[1:10]
level_order = url_freq$url[1:10]
ggplot(data=url_freq[1:10,], aes(x= factor(url, level = level_order), y=freq)) +
geom_col(fill='#00a3ad') +
ggtitle('Top 10 URL visited') +
ylab('Count of visits') +
xlab('URLs') +
theme(axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), panel.background = element_blank(),
axis.text.x=element_text(angle=45, hjust=1))
pop_url_viz = function(url_freq){
level_order = url_freq$url[1:10]
p = ggplot(data=url_freq[1:10,], aes(x= factor(url, level = level_order), y=freq)) +
geom_col(fill='#00a3ad') +
ggtitle('Top 10 URL visited') +
ylab('Count of visits') +
xlab('URLs') +
theme(axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), panel.background = element_blank(),
axis.text.x=element_text(angle=45, hjust=1))
return(p)
}
top10_url_plot = pop_url_viz(url_freq)
top10_url_plot
top10_url_plot
top10_url_plot = pop_url_viz(pop_url(df))
top10_url_plot
root_freq$url_root[1:10]
root_freq
level_order = root_freq$url_root[1:10]
level_order
ggplot(data=root_freq[1:10,], aes(x= factor(url_root, level = level_order), y=freq)) +
geom_col(fill='#00a3ad') +
ggtitle('Top 10 url/root visited') +
ylab('Count of visits') +
xlab('Root') +
theme(axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), panel.background = element_blank(),
axis.text.x=element_text(angle=45, hjust=1))
pop_root_viz = function(root_freq){
level_order = root_freq$url_root[1:10]
p = ggplot(data=root_freq[1:10,], aes(x= factor(url_root, level = level_order), y=freq)) +
geom_col(fill='#00a3ad') +
ggtitle('Top 10 url/root visited') +
ylab('Count of visits') +
xlab('Root') +
theme(axis.line = element_line(colour = "black"), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), panel.background = element_blank(),
axis.text.x=element_text(angle=45, hjust=1))
return(p)
}
source('web_content.R')
top10_url_plot = pop_url_viz(pop_url(df))
top10_root_plot = pop_root_viz(pop_root(df))
grid.arrange(top10_url_plot, top10_root_plot, ncol = 1)
grid.arrange(top10_root_plot, top10_url_plot, ncol = 1)
grid.arrange(top10_root_plot, top10_url_plot, ncol = 2)
# 0. Set up the environment and grab data for analysis-----
rm(list = ls())
options(java.parameters = "-Xmx8000m")
library(dplyr)
library(data.table)
source('extract_data.R')
options(scipen = 999)
# Using extract_data function to grab data from raw NASA web log files
df = extract_data('../../Data/nasa_19950630.22-19950728.12.tsv.gz')
df2 = extract_data('../../Data/nasa_19950731.22-19950831.22.tsv.gz')
df = rbind(df, df2)
rm(df2)
# 1. Check NASA web average traffic over 24 hours-----
# Using web_traffic function to count logged activities in each hour of the day
# Then using web_traffic_viz function to visualise the frequency of logged activities
source('web_traffic.R')
web_plot = web_traffic_viz(web_traffic(df))
web_ot_plot = web_traffic_ot_viz(web_traffic_ot(df))
grid.arrange(web_plot, web_ot_plot, ncol = 1)
# There is a surge in web traffic on 13th July 1995. Upon research it was discovered that there was a launch on that
# particular day for space shuttle Discovery with a crew of 5 from Florida.
# We can also see that there is a couple of days missing from the dataset, 29-31 July and 2 Aug.
# 2.
source('web_content.R')
top10_url_plot = pop_url_viz(pop_url(df))
top10_root_plot = pop_root_viz(pop_root(df))
top10_root_plot
top10_url_plot
